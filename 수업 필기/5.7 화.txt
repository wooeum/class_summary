feature_importances_ 조회하는 것 중요.


결정나무 회귀, graphviz   => 보스턴 집값 예측으로 결정나무 회귀를 사용하고 graphviz는 그래프 형태로 시각화할 수 있게 해주는 api이다.
source  =>시각화
print_regression_metrcis 

-----------------------------------------

의사결정나무는 분석 결과를 해석하기 쉬우며, 변수 간의 상호작용과 비선형성을 반영하여 정확한 예측이 가능하다는 장점이 있다. 또한 모델의 직관성과 가시성이 높아, 어떤 결정에 대한 이유를 설명하기 용이하다는 특징이 있다.
 그러나 과적합문제가 발생할 수 있고, 노이즈가 포함된 경우 정확한 예측이 어려울 수 있다는 한계점도 있다
=> 비즈니스 분야에서는 마케팅, 고객 분석, 고객 이탈 예측 등에 활용되고, 의료분야에서는 질병 예측, 진단, 치료법 추천등에 적용된다. 또한, 의사결정나무는 앙상블 기법 중 하나인 랜덤포레스트의 기본 요소로 사용되기도 한다

앙상블은 느린데 랜덤포레스트는 빠른 편인 이유?

앙상블 모델은 개별 모델로 실행했을떄 원하는 결과가 나오지 않았을때 여러 모델을 결합해 더 복잡한 모델을 만들어낸 것.

y에 시리즈를 그대로 넣어줘도 됨

디시젼트리는 원핫인코딩보단 레이블 인코딩이 낫다
결정나무에서 나누기엔 값들이 많을수록 좋다. 원핫인코딩을 하면 레이블은 0과 1로만 나뉘기때문에 

data scaling이 필요한 이유는 각 feature의 값들이 모델에 영향을 끼칠 수 있으므로 => feature를 통일시켜준다.
하지만 결정나무는 한 컬럼만 가지고 분류를 하므로 굳이 스케일링을 할 필요가 없다.

ada, gradient 가 대표적인 부스팅 방법이지만 ada는 이제 잘 안씀

오차를 줄여나가는 실행방식

정답을 예측하는게 아니라 잔차를 학습시킴

부스팅 모델은 복잡하면 안되고 덜 정확한 단순한 모델을 사용해야됨
max_depth를 줄이는 것과 비슷한 느낌 

max_depth와 같은 규제를 다른 클래스로 뺴서 표현해서 따로 설정해주지 않는다.

음수와 양수는 상관없다. 절댓값이 클수록 영향을 많이 줌

언더피팅났을때 =>  feature수 늘리기, 규제 약화
오버피팅          =>  feature수 줄이기, 규제 강화
