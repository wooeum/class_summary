전처리시에 원핫 인코딩이나 피쳐 스케일링을 해야한다

리니어에서도 오버피팅과 언더피팅이 있는데
오버피팅은 피쳐가 많아서. 언더는 피쳐가 적어서


### 언더피팅일때
파생변수를 통해서 feature를 늘린다
늘리는 방법 => polynomial features (제곱, 컬럼끼리의 곱:x1*x2)

### 오버피팅일때
# 실제로 줄임
f.selection => 중요한 것만 뽑아내기
f.extraction =>
# weight를 0에 가깝게 만들어줌 
규제 - lasso, rdge, en~ => 얘네들은 규제를 하는 방식이 다른것

L.R , Ridge, Lasso, E.N => weight를 찾는 방식이 다른거지(손실함수를 구하는 방법이 다름) 모델은 wx+b로 같다.  => 회귀모델

L.R을 기반으로하는 분류모델 => LogisticRegression => 이진분류모델

로지스틱에서 0.5 초과 => positive, 0.5까지는 neg

최적화를 한다 => 모델을 학습시켜 최적의 파라미터를 찾는다
최적의 파라미터는 예측시 오차를 가장 적게 만드는 파라미터
오차를 계산하는 함수 = loss function, cost function
loss function에 들어가는 파라미터 중 오차가 가장적은 파라미터를 찾아야되는데 이때 사용되는 것이 두가지가 있는데 하나는 함수, 하나는 경사하강법

손실함수의 도함수를 구하고 w값을 조절해 도함수의 값이 0이 되는 지점을 찾는다

LogisticRegression 주요 하이퍼 파라미터
penalty : linear와 똑같다. l1, l2, elasticnet, none
C: 작을수록 규제가 강하다
max_iter : 경사하강법 반복횟수

------------------------------------------------------------------
14. 군집

PCA나 군집은 보통 모델의 성능을 높이기 위해 보통 전처리 단계에서 사용한다
ex> 데이터를 그룹화해서 특정 컬럼을 만들때
=> 분류와 다름  -> 들어오는 새로운 데이터이 어디에 속하는지를 알려주기위해
하지만 군집은 지금 있는 데이터들을 분류하기 위한것

KMeans는 이상치에 취약한 모델이다. 그래서 전처리가 필요함

Inertia value(응집도)를 이용해서 적정한 군집수를 판단한다
응집도 => 센터점과 각 데이터와의 거리의 합 = k를 몇으로 정할지. 평가지표는 아님

이미 잘 나뉜경우 더 나눠도 줄어드는 거리의 비율이 작아질 것이므로 변화량을 통해 적정수를 구해준다

평가지표를 보는데 정답이 없는 비지도 학습에서는 애매하다.
하지만 수치로 보여주기위해 만들어진게 실루엣 점수
자기 군집과 얼마나 가깝고 다른 군집과는 얼마나 먼지를 나타냄
-1에서 1 사이. 1에 가까우면 자신이 속한 그룹과 가까운 것

