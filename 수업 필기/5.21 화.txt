분류에서도 MSELoss를 써도되는데 성능이 잘안나올뿐이다.

파라미터를 복사해서 가지고있는게 아니라 주소값이 전달되어 같은 값을 다룬다. shallow copy?
deep copy => 복사한 것을 전달해줌. 서로의 값에 영향을 끼치지 않는다

이 파일에서만 쓰는게 아니면 %%writefile을 사용해서 저장해두기

EPOCHS처럼 변수를 대문자로 선언해준경우 변경하지 말라는 뜻이다

과대적합, 과소적합인지 구분짓는 방법은 학습시에 성능을 통해 확인할 수 있다.

오버피팅 -> 모델의 성능이 너무 좋아서 쓸데없는 특징(우리문제와 상관없는 특징)들까지 찾아내 학습데이터에 너무 맞춰진 상태이다

딥러닝도 하나의 양상블 개념이다

linear(0, 1024) => 1024개의 리니어 리그레션을 앙상블하겠다 그리고 층이 하나 이상이므로 이보다 더하다

딥러닝은 오버피팅에 취약해 어떻게 해야 오버피팅을 줄일 수 있을지를 연구한다

다중분류 loss에서 pred는 logsoftmax, y는 원핫인코딩을 해주므로 레이블값을 그대로 넣어주면 된다

sequential에 넣어서 블럭화를 시켜주면
층을 여러번 쌓는 과정에서 여러줄의 코드를 작성해야되는데 sequential에 넣으면 바로 결과가 나오게 해줄 수 있다.

선형회귀에서는 각각의 피쳐간에 스케일을 맞춰주는게 좋다.

마지막 layer에서는 batchnorm을 정의할 필요가 없는데 여기서의 출력을 다음 입력으로 사용하지 않기때문이다